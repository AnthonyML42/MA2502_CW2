---
title: "NBA Analysis"
author: "| Obi-Wan Kenobi (C1234567) and Luke Skywalker (C2345678)\n| More names and
  student numbers (eventually)\n"
fontsize: 11pt
output:
  bookdown::pdf_document2:
    toc: true
    number_sections: true
    keep_tex: true
    citation_package: natbib
    fig_caption: true
    highlight: haddock
    df_print: kable
    extra_dependencies:
      caption: labelfont={bf}
  bookdown::html_document2:
    toc: true
    number_sections: true
    keep_tex: true
    citation_package: natbib
    fig_caption: true
    highlight: haddock
    df_print: kable
  html_document:
    toc: true
    df_print: paged
  pdf_document:
    toc: true
geometry: margin=1in
fontfamily: times
abstract: Write your abstract here.
---

<!-- set knitr options here -->

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
## Do not include warning messages in the report 
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


<!-- the report starts here
In the first R cell below, you should load all the packages and data sets used to generate the report. 
-->

```{r libraries and data import, include=FALSE}
## Packages
library(tidyverse)
library(stringi)
library(pROC)
library(matrixStats)
library(glmnet)
library(broom)
library(glue)
#library(stargazer)
library(texreg)

## Data
player_info <- read_csv("Data/Player_Info.csv")
player_salary <- read_csv("Data/Player_Salary.csv")
player_info <- player_info %>% mutate(PLAYER_NAME = paste(First_Name, Surname))
player_salary <- player_salary %>% rename(PLAYER_NAME = Name)
clean_shots <- read_csv("Data/NBA_Shots_Clean_Example.csv")
common_player_info <- read_csv("Data/wyatt_basketball/csv/common_player_info.csv")
common_player_info <- common_player_info %>% mutate(PLAYER_NAME = paste(first_name, last_name))
height_2014 <- read_csv("Data/NBA-Height-Weight/CSVs/Yearly/2014.csv") #https://github.com/simonwarchol/NBA-Height-Weight
height_2014 <- height_2014 %>% rename(PLAYER_NAME = Name)
```

```{r data name cleaning, include=FALSE}
# Data cleaning for consistency across datasets

typos <- c(
    "Time Hardaway Jr" = "Tim Hardaway Jr",
    "Steve Adams" = "Steven Adams",
    "Jose Juan Barea" = "Jj Barea",
    "Glen Rice Jr" = "Glen Rice",
    "Charles Hayes" = "Chuck Hayes", # technically correct but more sources with chuck
    "Ishmael Smith" = "Ish Smith", # as above
    "Patrick Mills" = "Patty Mills", # etc
    "Na Nene" = "Nene",
    "Jose Barea" = "Jj Barea"
)

# trying stuff to make this function a bit quicker
replace_strings <- function(df, replacements, cols = "PLAYER_NAME") {
    patterns <- names(replacements)
    replacement_values <- unname(replacements)

    df %>% mutate(
        across(all_of(cols), ~ stri_replace_all_fixed(., patterns, replacement_values, vectorize_all = FALSE))
    )
}


# Trying to standardise naming, works in almost every case!
clean_name <- function(name) {
    name %>%
        stri_replace_all_fixed("-", " ") %>%
        stri_replace_all_fixed("'", "") %>%
        stri_replace_all_fixed(".", "", vectorize_all = FALSE) %>%
        str_to_title()
}

player_info <- player_info %>%
    mutate(PLAYER_NAME = clean_name(PLAYER_NAME))

clean_shots <- clean_shots %>%
    mutate(PLAYER_NAME = clean_name(PLAYER_NAME),
          CLOSEST_DEFENDER = clean_name(CLOSEST_DEFENDER))

common_player_info <- common_player_info %>%
    mutate(PLAYER_NAME = clean_name(PLAYER_NAME)) %>%
    filter(person_id != 779) # filtering out glen rice sr. by hand

height_2014 <- height_2014 %>%
    mutate(PLAYER_NAME = clean_name(PLAYER_NAME))

clean_shots <- replace_strings(clean_shots, typos, c("PLAYER_NAME", "CLOSEST_DEFENDER"))
player_info <- replace_strings(player_info, typos)
common_player_info <- replace_strings(common_player_info, typos)
height_2014 <- replace_strings(height_2014, typos)

# Explicit case because of his name and interactions with the regex
player_info <- player_info %>% mutate(across(where(is.character), ~ str_replace_all(., "Luc Mbah", "Luc Mbah A Moute")))

```


# Introduction {#sec:Intro}

Lorem ipsum dolor sit amet. In rerum labore et quasi nobis est error quia eos numquam quod aut quaerat officia vel accusamus perspiciatis et labore quas. In atque delectus qui illo aliquid ut labore perferendis! 

# Predictors of shot success {#sec:Sec1}

In this section we investigate the relationship between variables in the dataset and shot success. Fitting logistic models to the data allows us to see the weightings of each predictor, check if their effect is significant and compare how they affect the log-odds of shot success. We will fit the first models on a training subset of the data, and then evaluate their predictive performance on a testing subset. Rather than strictly to analyse predictive performance, these metrics will show that shot success cannot fully be explained by the data we have - there are many more factors and basketball is a complex game!

## Naive logistic regression {#sec::Subsec1.1}

First we choose predictors from the data based on intuitive information about basketball. Our first logistic model predicts `SUCCESS` by the predictors listed in the call section below, along with a summary of the coefficient weights:

```{r height conversion function, include=FALSE}
# Heights like 6-4 are very annoying, convert them to cm here!

convert_to_cm <- function(feet_inches) {
  split_height <- strsplit(feet_inches, "-")
  
  feet <- sapply(split_height, function(x) as.numeric(x[1]))
  inches <- sapply(split_height, function(x) as.numeric(x[2]))
  
  cm_height <- (feet * 30.48) + (inches * 2.54)
  
  return(cm_height)
}
convert_to_cm <- Vectorize(convert_to_cm)
```

```{r height data joining and model data splitting, include=FALSE}

predictors <- c("GAME_ID", "PLAYER_NAME", "CLOSEST_DEFENDER" ,"SHOT_DIST", "PTS_TYPE",
                "CLOSE_DEF_DIST", "SHOT_CLOCK", "TOUCH_TIME", "PERIOD", "DRIBBLES", "LOCATION", "SUCCESS")

distinct_players_and_defenders <- union(
    clean_shots %>% distinct(PLAYER_NAME),
    clean_shots %>% distinct(CLOSEST_DEFENDER) %>% rename(PLAYER_NAME = CLOSEST_DEFENDER)
)

player_height_pos <- distinct_players_and_defenders %>%
    left_join(player_info %>% select(PLAYER_NAME, Height, Pos), by="PLAYER_NAME") %>%
    rename(H1 = Height)

player_height_pos <- player_height_pos %>%
    left_join(common_player_info %>% select(PLAYER_NAME, height, position), by="PLAYER_NAME", relationship = "many-to-many") %>%
    rename(H2 = height)
   
player_height_pos <- player_height_pos %>%
    left_join(height_2014 %>% select(PLAYER_NAME, "Height(Feet-Inches)"), by="PLAYER_NAME") %>%
    rename(H3 = "Height(Feet-Inches)")

# here is the opportunity for the NA height analysis!

player_height_pos <- player_height_pos %>%
    mutate(
    H2 = ifelse(!is.na(H2), convert_to_cm(H2), NA),
    H3 = ifelse(!is.na(H3), convert_to_cm(H3), NA),
    HEIGHT = case_when(
        !is.na(H1) ~ H1,
        is.na(H1) ~ rowMeans2(cbind(H2, H3), na.rm = TRUE),
        TRUE ~ NA_real_
    ))

# Finally join the "more complete" height data onto the shot data.
# Double join to get the defenders height data too - trick with renaming columns.
# After we have a complete model dataset here, we can split it up and only then we can convert to factors.

model_data_clean <- clean_shots %>% 
    select(all_of(predictors)) %>%
    left_join(player_height_pos %>% select(PLAYER_NAME, HEIGHT), by="PLAYER_NAME", relationship = "many-to-many") %>%
    rename(SHOOTER_HEIGHT = HEIGHT) %>%
    left_join(player_height_pos %>% select(PLAYER_NAME, HEIGHT) %>% rename(CLOSEST_DEFENDER = PLAYER_NAME), 
        by="CLOSEST_DEFENDER", relationship = "many-to-many") %>%
    rename(DEFENDER_HEIGHT = HEIGHT) %>%
    mutate(SHOOTER_HEIGHT_ADV = SHOOTER_HEIGHT - DEFENDER_HEIGHT) %>%
    filter(PERIOD <= 4)

# another chance for missing players

model_data_clean <- na.omit(model_data_clean)

# make test and train split (for initial models only)

model_data <- model_data_clean %>%
    mutate(across(c("GAME_ID", "PLAYER_NAME", "CLOSEST_DEFENDER", "PERIOD", "SUCCESS", "PTS_TYPE"), as.factor))
    
model_data_scaled <- model_data %>% mutate(across(where(is.numeric), scale))

set.seed(0)

# Base r approach for model test/train split. Avoids using caret
train_indices <- sample(1:nrow(model_data), size = 0.75 * nrow(model_data))
train_data <- model_data_scaled[train_indices, ]
test_data <- model_data_scaled[-train_indices, ]
```

```{r naive log model fit and summary, include=TRUE, echo=FALSE}
log_model <- glm(SUCCESS ~ SHOT_DIST + CLOSE_DEF_DIST + TOUCH_TIME + SHOOTER_HEIGHT_ADV + PERIOD + SHOT_CLOCK + DRIBBLES,  
                 data=train_data, family=binomial(link="logit"))

summary(log_model)
```

(Note that `PERIOD1` and some other variables are not present in the coefficient summary. This is because R considers the first factor in categorical data as a base, and then compares the others to it.)

From the coefficients table we can see weights (Estimate column) and whether the effect is significant or not (`Pr(>|z|)` column). In this model the period is not a significant predictor of shot success. On the other hand, shot distance is the largest predictor for decreasing shot success and the weight ($-0.55$) is somewhat larger than that of the other predictors.

One conclusion which is initially confusing is the positive weight of `CLOSE_DEF_DIST`, seemingly implying that a defender being closer makes a successful shot more likely. However when defenders are closest is usually when a layup is made near the net, which isn't necessarily harder than a wide-open 3 pointer. We will try to address this by splitting the models up by shot type later.

The remaining predictors have expected weight signs. The weight for `SHOOTER_HEIGHT_ADV` being positive but small tells us being taller than the nearest defender increases the likelihood of a successful shot. Similarly the longer the ball is held by the shooter (`TOUCH_TIME`), the lower the likelihood also.

Looking at the dataset columns and the predictors, we decided to exclude `PTS_TYPE` and `LOCATION` (home or away) because subsequent analysis will be dedicated to these factors.

## Regularised logistic regression {#sec::Subsec2.2}

Next we ask if these weights can truly be used in isolation to determine important factors relating to shot success? Refitting this model with a different set of predictors will yield different weights, possibly throwing our conclusions into question. We can add $\ell^1$ regularisation (specifically the penalty term $\lambda\sum_{i=1}^n{|\beta_{i}|}$) to regression models, which can shrink the coefficients of less powerful predictors to zero - hence the name lasso regression. For this model we have computed the **AUROC** (Area Under Receiver Operating Characteristic curve) which is a measure of how accurately the model can predict a successful shot. This metric is useful because a random classifier has an AUROC of $0.5$ which serves as a baseline. We see our AUROC is only $\approx0.6$, so only slightly better than random predictions! This low value further suggests that the data we have cannot explain the whole story behind shot success, there are so many more factors at play. We include the full ROC curve in the appendix also (link).

```{r log lasso model fitting and roc, include=FALSE}
x_train <- model.matrix(SUCCESS ~ SHOT_DIST + CLOSE_DEF_DIST + TOUCH_TIME + 
                        SHOOTER_HEIGHT_ADV + PERIOD + SHOT_CLOCK + DRIBBLES + PTS_TYPE + LOCATION, data = train_data)[, -1]
y_train <- train_data$SUCCESS

x_test <- model.matrix(SUCCESS ~ SHOT_DIST + CLOSE_DEF_DIST + TOUCH_TIME + 
                        SHOOTER_HEIGHT_ADV + PERIOD + SHOT_CLOCK + DRIBBLES + PTS_TYPE + LOCATION, data = test_data)[, -1]
y_test <- test_data$SUCCESS
lambda_opt <- 0.01828452 # hardcoded 10x CV lambda.1se precomputed (long compile times)

log_lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha=1, lambda=lambda_opt, standardize=FALSE) 

pred_probs <- predict(log_lasso_model, newx = x_test, type = "response", s = lambda_opt)
roc_obj <- roc(y_test, as.vector(pred_probs))
ci_auc <- round(ci.auc(roc_obj), 3)
auc_ci_result <- glue("{round(auc(roc_obj), 3)} [{ci_auc[1]}-{ci_auc[3]}]")
```

```{r lasso model summary, include=TRUE, echo=FALSE, fig.cap="AAAAAAAAAAAAAAAAAAA"}
# Model coefficients
log_lasso_model$beta
```

```{r lasso model auroc, include=TRUE}
# AUROC of the log lasso model and CI
auc_ci_result
```

(SEND ROC CURVE TO APPENDIX)

```{r roc-curve-appendix, include=TRUE, echo=FALSE}
roc(y_test, as.vector(pred_probs),
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
```

From this we see the most powerful negative predictors of shot success are `SHOT_DIST` and `TOUCH_TIME`, and the most powerful positive predictors are `SHOT_CLOCK` and `CLOSE_DEF_DIST`.


## Comparison of models using shot type and superstar status {#sec::Subsec2.3}

In order to gain further insights about shot success, we fit multiple models on different subsets of players. Specifically we fit a model for point type (2 or 3 point shots) and then by "superstar" status. In this case, we take superstars to be the most prolific 2 and 3 point scorer for each team. We see that the superstars account for roughly half of all the 3 pointer shots, but only $\approx 26\%$ for 2 point shots. This shows that overall each team relies heavily on one or two shooters from the three-point line. The plot of model coefficients with associated confidence intervals is shown below:

```{r data setup for split shot models, include=FALSE}
#superstars = c("Steph Curry", "Anthony Davis", "Lebron James", "James Harden", "Russel Westbrook", "Kyrie Irving",
#              "Demarcus Cousins", "Klay Thompson", "Dwayne Wade", "Damian Lillard") # This is arbitrary, so this is just a few good players.

super_3pt <- clean_shots %>% filter(PTS_TYPE == 3) %>% 
    group_by(PLAYER_NAME) %>% 
    summarise(shots_in = sum(SUCCESS==1)) %>%
    left_join(player_info %>% select(PLAYER_NAME, Team), by="PLAYER_NAME") %>%
    group_by(Team) %>%
    slice_max(order_by = shots_in, n=1) %>%
    arrange(desc(shots_in)) %>%
    pull(PLAYER_NAME)
super_2pt <- clean_shots %>% filter(PTS_TYPE == 2) %>% 
    group_by(PLAYER_NAME) %>% 
    summarise(shots_in = sum(SUCCESS==1)) %>%
    left_join(player_info %>% select(PLAYER_NAME, Team), by="PLAYER_NAME") %>%
    group_by(Team) %>%
    slice_max(order_by = shots_in, n=1) %>%
    arrange(desc(shots_in)) %>%
    pull(PLAYER_NAME)



model_data_2pt <- model_data_scaled %>% filter(PTS_TYPE == 2 & !PLAYER_NAME %in% super_2pt & !PLAYER_NAME %in% super_3pt)
model_data_3pt <- model_data_scaled %>% filter(PTS_TYPE == 3 & !PLAYER_NAME %in% super_2pt & !PLAYER_NAME %in% super_3pt)
model_data_super_2pt <- model_data_scaled %>% filter(PLAYER_NAME %in% super_2pt)
model_data_super_3pt <- model_data_scaled %>% filter(PLAYER_NAME %in% super_3pt)

# Repetitive but only to make sure the models are correct. A more concise syntax can be used if I filtered the data to only have the predictor cols.

log_model_2pt <- glm(SUCCESS ~ SHOT_DIST + CLOSE_DEF_DIST + TOUCH_TIME + SHOOTER_HEIGHT_ADV  + DRIBBLES + SHOT_CLOCK, 
                     data = model_data_2pt, family = binomial(link = "logit"))
log_model_3pt <- glm(SUCCESS ~ SHOT_DIST + CLOSE_DEF_DIST + TOUCH_TIME + SHOOTER_HEIGHT_ADV + DRIBBLES + SHOT_CLOCK, 
                     data = model_data_3pt, family = binomial(link = "logit"))
log_model_super_2pt <- glm(SUCCESS ~ SHOT_DIST + CLOSE_DEF_DIST + TOUCH_TIME + SHOOTER_HEIGHT_ADV + DRIBBLES + SHOT_CLOCK, 
                     data = model_data_super_2pt, family = binomial(link = "logit"))
log_model_super_3pt <- glm(SUCCESS ~ SHOT_DIST + CLOSE_DEF_DIST + TOUCH_TIME + SHOOTER_HEIGHT_ADV + DRIBBLES + SHOT_CLOCK, 
                     data = model_data_super_3pt, family = binomial(link = "logit"))

```

<style>
p.caption {
  font-size: 0.8em;
}
</style>

```{r split-model-dot-whisker-plot, include=TRUE, out.width='100%'}
#| echo=FALSE,
#| fig.width=12, 
#| fig.height=8,
#| fig.cap="Plot of model coefficients for shot type and superstars vs. other players. Lines around the points show Wald confidence intervals."
coefs_plot <- bind_rows(
    tidy(log_model_2pt)   %>% mutate(model = glue("2PT, n={nrow(model_data_2pt)}")),
    tidy(log_model_3pt)   %>% mutate(model = glue("3PT, n={nrow(model_data_3pt)}")),
    tidy(log_model_super_2pt) %>% mutate(model = glue("Superstar 2PT, n={nrow(model_data_super_2pt)}")),
    tidy(log_model_super_3pt) %>% mutate(model = glue("Superstar 3PT, n={nrow(model_data_super_3pt)}")),
  ) %>%
  filter(term != "(Intercept)") %>%
  mutate(
    # Wald CI here, 1.96 is a normal dist value.
    lower = estimate - 1.96 * std.error,
    upper = estimate + 1.96 * std.error,
    sig   = p.value <= 0.05
  )
ggplot(coefs_plot, aes(x = estimate, y = term, colour = model, shape = sig)) +
  geom_vline(xintercept = 0, linetype = "dashed", colour = "grey50") +
  geom_errorbarh(aes(xmin = lower, xmax = upper),
                 height = 0.2,
                 linewidth = 0.5,
                 position = position_dodge(width = 0.7)) +
  geom_point(position = position_dodge(width = 0.7), size = 2) +
  scale_color_brewer(palette = "Dark2") +
  scale_shape_manual(
    values = c(`FALSE` = 1, `TRUE` = 16),
    labels = c(`FALSE` = "Not significant", `TRUE` = "Significant")
  ) +
  labs(
    title  = "Model Coefficients by shot type and superstar status",
    x      = "Coefficient estimate",
    y      = NULL,
    colour = "Model",
    shape  = "Significance",
    
  ) +
  theme_light(base_size = 12) +
  theme(
    panel.grid.major.y = element_blank(),
    legend.position    = "bottom"
  ) +
     guides(color = guide_legend(nrow = 2))

```

Looking at the coefficients for `SHOT_DIST` we can see firstly that distance doesn't affect superstar players as much as other players. Secondly, the effect on those other players is much more varied from the visibly larger confidence interval.
